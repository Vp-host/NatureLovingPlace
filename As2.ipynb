{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a6a184",
   "metadata": {},
   "source": [
    "Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on data. \n",
    "Create embeddings using Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e078fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335a15bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vishal\n",
      "[nltk_data]     Pattar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d940439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"New Delhi is the capital city of India.\",\n",
    "    \"Mumbai is the Financial captial of India.\",\n",
    "    \"Early to bed, early to rise, makes a man health, wealthy and wise.\",\n",
    "    \"Failure is the stepping stone to success or every successful person once failed.\",\n",
    "    \"Be Honest to yourself, the world is yours\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df3d6296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bag of Word (Count) ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vocabulury:\n",
      " ['and' 'be' 'bed' 'capital' 'captial' 'city' 'delhi' 'early' 'every'\n",
      " 'failed' 'failure' 'financial' 'health' 'honest' 'india' 'is' 'makes'\n",
      " 'man' 'mumbai' 'new' 'of' 'once' 'or' 'person' 'rise' 'stepping' 'stone'\n",
      " 'success' 'successful' 'the' 'to' 'wealthy' 'wise' 'world' 'yours'\n",
      " 'yourself']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BoW Matrix (Count):\n",
      " [[0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 2 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 2 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"--- Bag of Word (Count) ---\")\n",
    "print(\"-\"*100)\n",
    "print(\"Vocabulury:\\n\", count_vectorizer.get_feature_names_out())\n",
    "print(\"-\"*100)\n",
    "print(\"BoW Matrix (Count):\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b78e7dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bag of Word (Normalized Count) ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BoW Matrix (Normalized Count):\n",
      " [[0.         0.         0.         0.125      0.         0.125\n",
      "  0.125      0.         0.         0.         0.         0.\n",
      "  0.         0.         0.125      0.125      0.         0.\n",
      "  0.         0.125      0.125      0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.125\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.14285714 0.\n",
      "  0.         0.         0.         0.         0.         0.14285714\n",
      "  0.         0.         0.14285714 0.14285714 0.         0.\n",
      "  0.14285714 0.         0.14285714 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.14285714\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.08333333 0.         0.08333333 0.         0.         0.\n",
      "  0.         0.16666667 0.         0.         0.         0.\n",
      "  0.08333333 0.         0.         0.         0.08333333 0.08333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.08333333 0.         0.         0.         0.         0.\n",
      "  0.16666667 0.08333333 0.08333333 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.07692308 0.07692308 0.07692308 0.\n",
      "  0.         0.         0.         0.07692308 0.         0.\n",
      "  0.         0.         0.         0.07692308 0.07692308 0.07692308\n",
      "  0.         0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.125      0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.125      0.         0.125      0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.125\n",
      "  0.125      0.         0.         0.125      0.125      0.125     ]]\n"
     ]
    }
   ],
   "source": [
    "normalized_bow = bow_matrix.toarray().astype(float)\n",
    "normalized_bow /= normalized_bow.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(\"--- Bag of Word (Normalized Count) ---\")\n",
    "print(\"-\"*100)\n",
    "print(\"BoW Matrix (Normalized Count):\\n\", normalized_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20f41011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TF-IDF ---\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vocabulury:\n",
      " ['and' 'be' 'bed' 'capital' 'captial' 'city' 'delhi' 'early' 'every'\n",
      " 'failed' 'failure' 'financial' 'health' 'honest' 'india' 'is' 'makes'\n",
      " 'man' 'mumbai' 'new' 'of' 'once' 'or' 'person' 'rise' 'stepping' 'stone'\n",
      " 'success' 'successful' 'the' 'to' 'wealthy' 'wise' 'world' 'yours'\n",
      " 'yourself']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.         0.41042134 0.         0.41042134\n",
      "  0.41042134 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33112535 0.23122423 0.         0.\n",
      "  0.         0.41042134 0.33112535 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.23122423\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.45007472 0.\n",
      "  0.         0.         0.         0.         0.         0.45007472\n",
      "  0.         0.         0.36311745 0.25356425 0.         0.\n",
      "  0.45007472 0.         0.36311745 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25356425\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.26924896 0.         0.26924896 0.         0.         0.\n",
      "  0.         0.53849791 0.         0.         0.         0.\n",
      "  0.26924896 0.         0.         0.         0.26924896 0.26924896\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.26924896 0.         0.         0.         0.         0.\n",
      "  0.36063833 0.26924896 0.26924896 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30037597 0.30037597 0.30037597 0.\n",
      "  0.         0.         0.         0.16922658 0.         0.\n",
      "  0.         0.         0.         0.30037597 0.30037597 0.30037597\n",
      "  0.         0.30037597 0.30037597 0.30037597 0.30037597 0.16922658\n",
      "  0.20116529 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.40544309 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.40544309 0.         0.22841956 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.22841956\n",
      "  0.27152996 0.         0.         0.40544309 0.40544309 0.40544309]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"--- TF-IDF ---\")\n",
    "print(\"-\"*100)\n",
    "print(\"Vocabulury:\\n\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"-\"*100)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f72e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37acbce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=tokenized_corpus, window=5, min_count=1, vector_size=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4556dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.30016683e-03 -9.80430283e-03  4.58776252e-03 -5.38222783e-04\n",
      "  6.33209571e-03  1.78347470e-03 -3.12979822e-03  7.75997294e-03\n",
      "  1.55466562e-03  5.52093989e-05 -4.61295387e-03 -8.45352374e-03\n",
      " -7.76683213e-03  8.67050979e-03 -8.92496016e-03  9.03471559e-03\n",
      " -9.28101782e-03 -2.76756298e-04 -1.90704700e-03 -8.93114600e-03\n",
      "  8.63005966e-03  6.77781366e-03  3.01943906e-03  4.83345287e-03\n",
      "  1.12190246e-04  9.42468084e-03  7.02128746e-03 -9.85372625e-03\n",
      " -4.43322072e-03 -1.29011157e-03  3.04772262e-03 -4.32395237e-03\n",
      "  1.44916656e-03 -7.84589909e-03  2.77807354e-03  4.70269192e-03\n",
      "  4.93731257e-03 -3.17570218e-03 -8.42704065e-03 -9.22061782e-03\n",
      " -7.22899451e-04 -7.32746487e-03 -6.81496272e-03  6.12000562e-03\n",
      "  7.17230327e-03  2.11741915e-03 -7.89940078e-03 -5.69898821e-03\n",
      "  8.05184525e-03  3.92084382e-03 -5.24047017e-03 -7.39190448e-03\n",
      "  7.71554711e-04  3.46375466e-03  2.07919348e-03  3.10080405e-03\n",
      " -5.62050007e-03 -9.88948625e-03 -7.02083716e-03  2.30308768e-04\n",
      "  4.61867917e-03  4.52630781e-03  1.87981245e-03  5.17067453e-03\n",
      " -1.05360748e-04  4.11416637e-03 -9.12324060e-03  7.70091172e-03\n",
      "  6.14747405e-03  5.12415636e-03  7.20666908e-03  8.43979698e-03\n",
      "  7.38695846e-04 -1.70386070e-03  5.18628338e-04 -9.31678060e-03\n",
      "  8.40621442e-03 -6.37993217e-03  8.42784252e-03 -4.24435502e-03\n",
      "  6.46842702e-04 -9.16416850e-03 -9.55856778e-03 -7.83681031e-03\n",
      " -7.73105631e-03  3.75581993e-04 -7.22646248e-03 -4.95021325e-03\n",
      " -5.27170673e-03 -4.28929785e-03  7.01231137e-03  4.82938997e-03\n",
      "  8.68277065e-03  7.09359162e-03 -5.69440611e-03  7.24079600e-03\n",
      " -9.29490291e-03 -2.58756871e-03 -7.75716640e-03  4.19260142e-03]\n",
      "[('.', 0.2189740538597107), ('early', 0.1748562455177307), ('india', 0.1637314260005951), ('successful', 0.142072394490242), ('to', 0.1086244136095047), ('world', 0.10757598280906677), ('wealthy', 0.10481765866279602), ('stepping', 0.09880394488573074), ('health', 0.09046750515699387), ('city', 0.06548558175563812)]\n"
     ]
    }
   ],
   "source": [
    "word = \"capital\"\n",
    "if word in w2v_model.wv:\n",
    "    print(w2v_model.wv[word])\n",
    "    print(w2v_model.wv.most_similar(word))\n",
    "else:\n",
    "    print(\"Word doesn't exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a241d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
